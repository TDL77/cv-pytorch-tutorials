{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "Ota0_qudYDa2",
   "metadata": {
    "id": "Ota0_qudYDa2"
   },
   "source": [
    "## Train image classifier on dogs cats dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DAs29Fl5Xg3O",
   "metadata": {
    "id": "DAs29Fl5Xg3O"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e0e189",
   "metadata": {
    "id": "c1e0e189"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36dlhsuYNv5",
   "metadata": {
    "id": "e36dlhsuYNv5"
   },
   "source": [
    "### Setup log folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59eda418",
   "metadata": {
    "id": "59eda418"
   },
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"cnn_dogscats\" \n",
    "ROOT_DIR = os.path.abspath(\".\")\n",
    "LOG_PATH = os.path.join(ROOT_DIR, \"logs\", EXPERIMENT_NAME)\n",
    "\n",
    "if not os.path.exists(os.path.join(ROOT_DIR, \"logs\")):\n",
    "    os.mkdir(os.path.join(ROOT_DIR, \"logs\"))\n",
    "    \n",
    "if not os.path.exists(LOG_PATH):\n",
    "    os.mkdir(LOG_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jIE-bGnhYg4z",
   "metadata": {
    "id": "jIE-bGnhYg4z"
   },
   "source": [
    "### Setup params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e56cb9f",
   "metadata": {
    "id": "6e56cb9f"
   },
   "outputs": [],
   "source": [
    "# Device\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)\n",
    "\n",
    "# Hyperparameters\n",
    "RANDOM_SEED = 1\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Architecture\n",
    "NUM_CLASSES = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nGcLm6R3ZVkw",
   "metadata": {
    "id": "nGcLm6R3ZVkw"
   },
   "source": [
    "### Download data and setup path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f390997",
   "metadata": {
    "id": "5f390997"
   },
   "outputs": [],
   "source": [
    "# Get data, use if only data is not available already\n",
    "# os.system(\"mkdir ./datasets\")\n",
    "# os.system(\"wget -O ./datasets/dogscats.zip https://github.com/hasibzunair/my-releases/releases/download/v1-datasets/dogscats.zip\")\n",
    "# os.system(\"unzip ./datasets/dogscats.zip -d ./datasets/\")\n",
    "\n",
    "# Setup training data\n",
    "TRAIN_ROOT = './datasets/dogscats/train'\n",
    "num_train_cats = len([i for i in os.listdir(os.path.join(TRAIN_ROOT, 'Cat')) \n",
    "                      if i.endswith('.jpg') and i.startswith('cat')])\n",
    "\n",
    "num_train_dogs = len([i for i in os.listdir(os.path.join(TRAIN_ROOT, 'Dog')) \n",
    "                      if i.endswith('.jpg') and i.startswith('dog')])\n",
    "\n",
    "print(f'Training set cats: {num_train_cats}')\n",
    "print(f'Training set dogs: {num_train_dogs}')\n",
    "\n",
    "# Setup validation data\n",
    "VAL_ROOT = './datasets/dogscats/val'\n",
    "num_train_cats = len([i for i in os.listdir(os.path.join(VAL_ROOT, 'Cat')) \n",
    "                      if i.endswith('.jpg') and i.startswith('cat')])\n",
    "\n",
    "num_train_dogs = len([i for i in os.listdir(os.path.join(VAL_ROOT, 'Dog')) \n",
    "                      if i.endswith('.jpg') and i.startswith('dog')])\n",
    "\n",
    "print(f'Validation set cats: {num_train_cats}')\n",
    "print(f'Validation set dogs: {num_train_dogs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XKppHUOqdPbW",
   "metadata": {
    "id": "XKppHUOqdPbW"
   },
   "source": [
    "### Plot and image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841e0e85",
   "metadata": {
    "id": "841e0e85"
   },
   "outputs": [],
   "source": [
    "img = Image.open(os.path.join(TRAIN_ROOT, 'Cat', 'cat.59.jpg'))\n",
    "print(np.asarray(img, dtype=np.uint8).shape)\n",
    "plt.imshow(img);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa0ab4b",
   "metadata": {
    "id": "5fa0ab4b"
   },
   "outputs": [],
   "source": [
    "img = Image.open(os.path.join(TRAIN_ROOT, 'Dog', 'dog.23.jpg'))\n",
    "print(np.asarray(img, dtype=np.uint8).shape)\n",
    "plt.imshow(img);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4bffd1",
   "metadata": {
    "id": "5c4bffd1"
   },
   "source": [
    "### Applying Imagefolder Class and Transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5788d05",
   "metadata": {
    "id": "c5788d05"
   },
   "outputs": [],
   "source": [
    "## setting directories\n",
    "data_dir = './datasets/dogscats/'\n",
    "train_dir = os.path.join(data_dir, 'train') \n",
    "val_dir = os.path.join(data_dir, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3ed980",
   "metadata": {
    "id": "da3ed980"
   },
   "outputs": [],
   "source": [
    "## transform\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), # resize image\n",
    "    transforms.CenterCrop((224, 224)), # center crop image\n",
    "    transforms.ToTensor(), # convert to Pytorch tensor\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # normalize\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.CenterCrop((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_dataset = ImageFolder(train_dir, train_transforms)\n",
    "val_dataset = ImageFolder(val_dir, val_transforms)\n",
    "test_dataset = ImageFolder(val_dir, val_transforms) # note: val and test is same\n",
    "\n",
    "print(f'Number of training examples: {len(train_dataset)}')\n",
    "print(f'Number of validation examples: {len(val_dataset)}')\n",
    "print(f'Number of test examples: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b74ca3",
   "metadata": {
    "id": "36b74ca3"
   },
   "outputs": [],
   "source": [
    "## layout of the dataset\n",
    "\n",
    "print(train_dataset.class_to_idx)\n",
    "print(val_dataset.class_to_idx)\n",
    "print(test_dataset.class_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0942eb5c",
   "metadata": {
    "id": "0942eb5c"
   },
   "source": [
    "### Setup dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258eba04",
   "metadata": {
    "id": "258eba04"
   },
   "outputs": [],
   "source": [
    "# train loader and test loader\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, drop_last=True,\n",
    "                          shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n",
    "                          shuffle=False)\n",
    "test_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n",
    "                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776d0bab",
   "metadata": {
    "id": "776d0bab"
   },
   "source": [
    "### Visualize data from data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3bb2e3",
   "metadata": {
    "id": "ff3bb2e3"
   },
   "outputs": [],
   "source": [
    "def to_img(ten):\n",
    "    # Un-normalize and convert to RGB format for visualization\n",
    "    ten =(ten[0].permute(1,2,0).detach().cpu().numpy()+1)/2\n",
    "    ten=(ten*255).astype(np.uint8)\n",
    "    return ten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee3fbae",
   "metadata": {
    "id": "cee3fbae"
   },
   "outputs": [],
   "source": [
    "cat = train_dataset[1] ## cat is tuple\n",
    "dog = train_dataset[1500] ## dog is a tuple, dog[0] holds its image tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9449dfeb",
   "metadata": {
    "id": "9449dfeb"
   },
   "outputs": [],
   "source": [
    "a = to_img(cat)\n",
    "print(a.shape)\n",
    "plt.imshow(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8e0f43",
   "metadata": {
    "id": "bc8e0f43"
   },
   "outputs": [],
   "source": [
    "a = to_img(dog)\n",
    "print(a.shape)\n",
    "plt.imshow(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591e5c31",
   "metadata": {
    "id": "591e5c31"
   },
   "source": [
    "### Define ConvNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48f0296",
   "metadata": {
    "id": "c48f0296"
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "### MODEL\n",
    "##########################\n",
    "\n",
    "\n",
    "class VGG16(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super(VGG16, self).__init__()\n",
    "        self.block_1 = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=3,\n",
    "                          out_channels=64,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          # (1(32-1)- 32 + 3)/2 = 1\n",
    "                          padding=1), \n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(in_channels=64,\n",
    "                          out_channels=64,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=(2, 2),\n",
    "                             stride=(2, 2))\n",
    "        )\n",
    "        \n",
    "        self.block_2 = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=64,\n",
    "                          out_channels=128,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(in_channels=128,\n",
    "                          out_channels=128,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=(2, 2),\n",
    "                             stride=(2, 2))\n",
    "        )\n",
    "        \n",
    "        self.block_3 = nn.Sequential(        \n",
    "                nn.Conv2d(in_channels=128,\n",
    "                          out_channels=256,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(in_channels=256,\n",
    "                          out_channels=256,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),        \n",
    "                nn.Conv2d(in_channels=256,\n",
    "                          out_channels=256,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(in_channels=256,\n",
    "                          out_channels=256,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=(2, 2),\n",
    "                             stride=(2, 2))\n",
    "        )\n",
    "        \n",
    "          \n",
    "        self.block_4 = nn.Sequential(   \n",
    "                nn.Conv2d(in_channels=256,\n",
    "                          out_channels=512,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),        \n",
    "                nn.Conv2d(in_channels=512,\n",
    "                          out_channels=512,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),        \n",
    "                nn.Conv2d(in_channels=512,\n",
    "                          out_channels=512,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(in_channels=512,\n",
    "                          out_channels=512,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),   \n",
    "                nn.MaxPool2d(kernel_size=(2, 2),\n",
    "                             stride=(2, 2))\n",
    "        )\n",
    "        \n",
    "        self.block_5 = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=512,\n",
    "                          out_channels=512,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),            \n",
    "                nn.Conv2d(in_channels=512,\n",
    "                          out_channels=512,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),            \n",
    "                nn.Conv2d(in_channels=512,\n",
    "                          out_channels=512,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(in_channels=512,\n",
    "                          out_channels=512,\n",
    "                          kernel_size=(3, 3),\n",
    "                          stride=(1, 1),\n",
    "                          padding=1),\n",
    "                nn.ReLU(),   \n",
    "                nn.MaxPool2d(kernel_size=(2, 2),\n",
    "                             stride=(2, 2))             \n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "                nn.Linear(512*7*7, 4096),\n",
    "                nn.ReLU(),   \n",
    "                nn.Linear(4096, 4096),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(4096, num_classes)\n",
    "        )\n",
    "            \n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Conv2d):\n",
    "                m.weight.detach().normal_(0, 0.05)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.detach().zero_()\n",
    "            elif isinstance(m, torch.nn.Linear):\n",
    "                m.weight.detach().normal_(0, 0.05)\n",
    "                m.bias.detach().detach().zero_()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.block_1(x)\n",
    "        x = self.block_2(x)\n",
    "        x = self.block_3(x)\n",
    "        x = self.block_4(x)\n",
    "        x = self.block_5(x)\n",
    "        logits = self.classifier(x.view(-1, 512*7*7))\n",
    "        probas = F.softmax(logits, dim=1)\n",
    "        return logits, probas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CwEYcqoXeBEU",
   "metadata": {
    "id": "CwEYcqoXeBEU"
   },
   "source": [
    "### Build model and define optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8aa254",
   "metadata": {
    "id": "1d8aa254"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "model = VGG16(num_classes=NUM_CLASSES)\n",
    "model = model.to(DEVICE)\n",
    "print(model)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96e0f40",
   "metadata": {
    "id": "c96e0f40"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZKVQTxMCi846",
   "metadata": {
    "id": "ZKVQTxMCi846"
   },
   "outputs": [],
   "source": [
    "def compute_accuracy_and_loss(model, data_loader, device):\n",
    "    correct_pred, num_examples = 0, 0\n",
    "    cross_entropy = 0.\n",
    "    for i, (features, targets) in enumerate(data_loader):\n",
    "            \n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        logits, probas = model(features)\n",
    "        cross_entropy += F.cross_entropy(logits, targets).item()\n",
    "        _, predicted_labels = torch.max(probas, 1)\n",
    "        num_examples += targets.size(0)\n",
    "        correct_pred += (predicted_labels == targets).sum()\n",
    "    return correct_pred.float()/num_examples * 100, cross_entropy/num_examples\n",
    "    \n",
    "\n",
    "start_time = time.time()\n",
    "train_acc_lst, valid_acc_lst = [], []\n",
    "train_loss_lst, valid_loss_lst = [], []\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "    \n",
    "        ### PREPARE MINIBATCH\n",
    "        features = features.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "            \n",
    "        ### FORWARD AND BACK PROP\n",
    "        logits, probas = model(features)\n",
    "        cost = F.cross_entropy(logits, targets)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cost.backward()\n",
    "        \n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### LOGGING, every 100 batches\n",
    "        if not batch_idx % 25: \n",
    "            print (f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} | '\n",
    "                   f'Batch {batch_idx:03d}/{len(train_loader):03d} |' \n",
    "                   f' Cost: {cost:.4f}')\n",
    "\n",
    "    # no need to build the computation graph for backprop when computing accuracy\n",
    "    model.eval()\n",
    "    with torch.set_grad_enabled(False):\n",
    "        train_acc, train_loss = compute_accuracy_and_loss(model, train_loader, device=DEVICE)\n",
    "        valid_acc, valid_loss = compute_accuracy_and_loss(model, val_loader, device=DEVICE)\n",
    "        \n",
    "        #Save best model\n",
    "        if valid_acc > best_acc:\n",
    "            print(\"Saving model at accuracy={:.3f}\".format(valid_acc))\n",
    "            torch.save(model.state_dict(), '{}/{}.pth'.format(LOG_PATH, 'model'))\n",
    "            best_acc = valid_acc\n",
    "            \n",
    "        train_acc_lst.append(train_acc)\n",
    "        valid_acc_lst.append(valid_acc)\n",
    "        train_loss_lst.append(train_loss)\n",
    "        valid_loss_lst.append(valid_loss)\n",
    "        print(f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} Train Acc.: {train_acc:.2f}%'\n",
    "              f' | Validation Acc.: {valid_acc:.2f}%')\n",
    "        \n",
    "    elapsed = (time.time() - start_time)/60\n",
    "    print(f'Time elapsed: {elapsed:.2f} min')\n",
    "\n",
    "elapsed = (time.time() - start_time)/60\n",
    "print(f'Total Training Time: {elapsed:.2f} min') # 20 min per epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1xyH_n0f2v8",
   "metadata": {
    "id": "a1xyH_n0f2v8"
   },
   "source": [
    "### Plot loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fccb41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_acc_lst, new_valid_acc_lst = [], []\n",
    "for i in train_acc_lst:\n",
    "    i = i.cpu()\n",
    "    new_train_acc_lst.append(i)\n",
    "for l in valid_acc_lst:\n",
    "    l = l.cpu()\n",
    "    new_valid_acc_lst.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4af373",
   "metadata": {
    "id": "ae4af373"
   },
   "outputs": [],
   "source": [
    "train_loss_lst = np.array(train_loss_lst)\n",
    "np.savetxt(\"{}/{}_train_loss.txt\".format(LOG_PATH, EXPERIMENT_NAME), train_loss_lst, delimiter=\",\")\n",
    "valid_loss_lst = np.array(valid_loss_lst)\n",
    "np.savetxt(\"{}/{}_valid_loss.txt\".format(LOG_PATH, EXPERIMENT_NAME), valid_loss_lst, delimiter=\",\")\n",
    "\n",
    "train_acc_lst = np.array(new_train_acc_lst)\n",
    "np.savetxt(\"{}/{}_train_acc.txt\".format(LOG_PATH, EXPERIMENT_NAME), train_acc_lst, delimiter=\",\")\n",
    "valid_acc_lst = np.array(new_valid_acc_lst)\n",
    "np.savetxt(\"{}/{}_valid_acc.txt\".format(LOG_PATH, EXPERIMENT_NAME), valid_acc_lst, delimiter=\",\")\n",
    "\n",
    "plt.plot(range(1, NUM_EPOCHS+1), train_loss_lst, label='Training loss')\n",
    "plt.plot(range(1, NUM_EPOCHS+1), valid_loss_lst, label='Validation loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross entropy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.savefig('{}/{}_loss_graph.png'.format(LOG_PATH, EXPERIMENT_NAME), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd98c81",
   "metadata": {
    "id": "0cd98c81"
   },
   "outputs": [],
   "source": [
    "plt.plot(range(1, NUM_EPOCHS+1), new_train_acc_lst, label='Training accuracy')\n",
    "plt.plot(range(1, NUM_EPOCHS+1), new_valid_acc_lst, label='Validation accuracy')\n",
    "plt.legend(loc='upper left')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.savefig('{}/{}_acc_graph.png'.format(LOG_PATH, EXPERIMENT_NAME), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432f805b",
   "metadata": {
    "id": "432f805b"
   },
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42b3434",
   "metadata": {
    "id": "b42b3434"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.set_grad_enabled(False): # save memory during inference\n",
    "    test_acc, test_loss = compute_accuracy_and_loss(model, val_loader, DEVICE)\n",
    "    print(f'Test accuracy: {test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26cb4f1",
   "metadata": {
    "id": "e26cb4f1"
   },
   "outputs": [],
   "source": [
    "class UnNormalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized image.\n",
    "        \"\"\"\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.mul_(s).add_(m)\n",
    "            # The normalize code -> t.sub_(m).div_(s)\n",
    "        return tensor\n",
    "    \n",
    "    \n",
    "train_mean, train_std = [0.5, 0.5, 0.5], [0.5, 0.5, 0.5]\n",
    "\n",
    "train_mean = torch.tensor(train_mean)\n",
    "train_std = torch.tensor(train_std)\n",
    "\n",
    "print(train_mean)\n",
    "print(train_std)\n",
    "\n",
    "unorm = UnNormalize(mean=train_mean, std=train_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1NV-SaAlgdz7",
   "metadata": {
    "id": "1NV-SaAlgdz7"
   },
   "source": [
    "### Visualize some predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdb341f",
   "metadata": {
    "id": "bfdb341f"
   },
   "outputs": [],
   "source": [
    "val_loader = DataLoader(dataset=train_dataset, \n",
    "                         batch_size=BATCH_SIZE, \n",
    "                         shuffle=True)\n",
    "\n",
    "for features, targets in val_loader:\n",
    "    break\n",
    "    \n",
    "\n",
    "_, predictions = model.forward(features[:8].to(DEVICE))\n",
    "predictions = torch.argmax(predictions, dim=1)\n",
    "\n",
    "d = {0: 'cat',\n",
    "     1: 'dog'}\n",
    "    \n",
    "fig, ax = plt.subplots(1, 8, figsize=(20, 10))\n",
    "for i in range(8):\n",
    "    img = unorm(features[i])\n",
    "    ax[i].imshow(np.transpose(img, (1, 2, 0)))\n",
    "    ax[i].set_xlabel(d[predictions[i].item()])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f996f87",
   "metadata": {
    "id": "7f996f87"
   },
   "outputs": [],
   "source": [
    "test_loader = DataLoader(dataset=val_dataset, \n",
    "                         batch_size=BATCH_SIZE, \n",
    "                         shuffle=True)\n",
    "\n",
    "for features, targets in test_loader:\n",
    "    break\n",
    "    \n",
    "\n",
    "_, predictions = model.forward(features[:8].to(DEVICE))\n",
    "predictions = torch.argmax(predictions, dim=1)\n",
    "\n",
    "d = {0: 'cat',\n",
    "     1: 'dog'}\n",
    "    \n",
    "fig, ax = plt.subplots(1, 8, figsize=(20, 10))\n",
    "for i in range(8):\n",
    "    img = unorm(features[i])\n",
    "    ax[i].imshow(np.transpose(img, (1, 2, 0)))\n",
    "    ax[i].set_xlabel(d[predictions[i].item()])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f781cd36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17c09724",
   "metadata": {},
   "source": [
    "### Infer on one input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f24353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "model_path = f'./logs/{EXPERIMENT_NAME}/model.pth'\n",
    "model = VGG16(num_classes=NUM_CLASSES)\n",
    "checkpoint = torch.load(model_path)\n",
    "model.load_state_dict(checkpoint)\n",
    "model = model.to(DEVICE)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22b31b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference!\n",
    "def inference(input_image_path):\n",
    "    image = Image.open(input_image_path).convert(\"RGB\")\n",
    "    class_index = {0: 'cat', 1: 'dog'}\n",
    "\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.CenterCrop((224, 224)),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    input_tensor = preprocess(image)\n",
    "    input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "\n",
    "    # Move the input and model to GPU for speed if available\n",
    "    if torch.cuda.is_available():\n",
    "        input_batch = input_batch.to('cuda')\n",
    "        model.to('cuda')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _, predictions = model.forward(input_batch)\n",
    "        \n",
    "    predictions = torch.argmax(predictions, dim=1)\n",
    "    label = class_index[predictions.item()]\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd639ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_img = os.path.join(TRAIN_ROOT, 'Dog', 'dog.25.jpg')\n",
    "#path_to_img = os.path.join(TRAIN_ROOT, 'Cat', 'cat.20.jpg')\n",
    "\n",
    "prediction = inference(path_to_img)\n",
    "print(f\"The predicted label is {prediction}.\")\n",
    "\n",
    "img = Image.open(path_to_img)\n",
    "print(np.asarray(img, dtype=np.uint8).shape)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4ca3c0",
   "metadata": {
    "id": "8f4ca3c0"
   },
   "source": [
    "### Evaluate on various metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3982e5",
   "metadata": {
    "id": "5c3982e5"
   },
   "outputs": [],
   "source": [
    "print(len(test_dataset))\n",
    "print(len(test_loader))\n",
    "print(test_dataset.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a86add",
   "metadata": {
    "id": "b2a86add"
   },
   "outputs": [],
   "source": [
    "def evaluation(model, data_loader, device):\n",
    "    correct_pred, num_examples = 0, 0\n",
    "    cross_entropy = 0.\n",
    "    predictions = []\n",
    "    tgts = []\n",
    "    for i, (features, targets) in enumerate(data_loader):\n",
    "            \n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        logits, probas = model(features)\n",
    "        cross_entropy += F.cross_entropy(logits, targets).item()\n",
    "        _, predicted_labels = torch.max(probas, 1)\n",
    "        num_examples += targets.size(0)\n",
    "        correct_pred += (predicted_labels == targets).sum()\n",
    "        predicted_labels = predicted_labels.detach().cpu().numpy()\n",
    "        predictions.extend(predicted_labels)\n",
    "        targets = targets.detach().cpu().numpy()\n",
    "        tgts.extend(targets)\n",
    "    return correct_pred.float()/num_examples * 100, cross_entropy/num_examples, tgts, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jH8UcmISg55V",
   "metadata": {
    "id": "jH8UcmISg55V"
   },
   "source": [
    "### Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac77f5c8",
   "metadata": {
    "id": "ac77f5c8"
   },
   "outputs": [],
   "source": [
    "## load best model\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "model_path = f'./logs/{EXPERIMENT_NAME}/model.pth'\n",
    "model = VGG16(num_classes=NUM_CLASSES)\n",
    "checkpoint = torch.load(model_path)\n",
    "model.load_state_dict(checkpoint)\n",
    "model = model.to(DEVICE)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vRqqgydCgr_3",
   "metadata": {
    "id": "vRqqgydCgr_3"
   },
   "source": [
    "### Get accuracy, precision, recall, AUC etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1869ec",
   "metadata": {
    "id": "5e1869ec"
   },
   "outputs": [],
   "source": [
    "with torch.set_grad_enabled(False): # save memory during inference\n",
    "    test_acc, test_loss, y_true, y_pred = evaluation(model, test_loader, DEVICE)\n",
    "    print(f'Test accuracy: {test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c3fd04",
   "metadata": {
    "id": "71c3fd04"
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb5ffec",
   "metadata": {
    "id": "4bb5ffec"
   },
   "outputs": [],
   "source": [
    "print(f'area under roc curve = {roc_auc_score(y_true, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2b3e88",
   "metadata": {
    "id": "bf2b3e88"
   },
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "print(f'TP = {tp}')\n",
    "print(f'FN = {fn}')\n",
    "print(f'FP = {fp}')\n",
    "print(f'TN = {tn}')\n",
    "\n",
    "sens = tp/(tp+fn) * 100 \n",
    "specfs = tn/(tn+fp) * 100 \n",
    "\n",
    "print(10*'=')\n",
    "print(f'Sensitivity = {sens}%')\n",
    "print(f'Specificity = {specfs}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bd8be9",
   "metadata": {},
   "source": [
    "### Acknowledgements\n",
    "\n",
    "Parts of this notebook were written by https://github.com/mahmudhasankhan."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "syngen_pipeline.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.8.12 ('fifa')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "5a4cff4f724f20f3784f32e905011239b516be3fadafd59414871df18d0dad63"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
